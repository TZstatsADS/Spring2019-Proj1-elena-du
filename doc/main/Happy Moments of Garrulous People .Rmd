---
title: "What makes people garrulous about their happiness?"
output:
  html_document:
    df_print: paged
---

### Introduction. When happy moments speak?

The database HappyDB (https://rit-public.github.io/HappyDB) contains over 100,000 crowd-sourced happy moments. Participants are asked to write one or more complete sentences on what made them happy in past 24 hours or 3 months. What drew my attention is that even though absolute majority come up with one sentence per happy moment, there are almost a thousand of happy moments over 5 sentences. The longest one consists of 69 sentences! There is a famous saying by Seneca "Curae leves loquuntur ingentes stupent" ("Slight griefs talk, great ones are speechless"). So, I wonder if there is also some interesting correlation between length of a "happy moment pitch" and its properties. Who are those people who write extensive essays, how they are different from people who prefer to get away with few sentences? What are the topics that encourage more talk?

The body of the analysis consists of two main parts: statistical analysis using demographic data and text mining (topic modeling).  

### Libraries and Data

Loading the libraries to be used for data manipulation, text mining and visualization.

```{r libraries, echo = FALSE, warning=FALSE, message=FALSE}

library(data.table)
library(DT)
library(dplyr)

library(tm)
library(tidyverse) 
library(tidytext) 
library(topicmodels) 
library(SnowballC) 
library(classInt)

library(ggplot2)
library(cowplot)
library(beeswarm)
library(rworldmap)
library(RColorBrewer)

```

```{r functions}

words = c("happy", "happiness", "happiest", "moment", "good", "bad", "past", "years", "favorite", "enjoy", "enjoyed", "year", "months", "weeks", "event", "made", "felt", "feel", "good", "ago", "great", "awesome", "love", "hate",  "today", "yesterday", "lot", "week", "finally", "day", "time", "times", "nice", "ive", "didnt", "amazing", "month", "iam", "youre", "wasnt", "make")

getCorpus = function(text) {
  corpus = VCorpus(VectorSource(text)) %>%
    tm_map(content_transformer(tolower)) %>%
    tm_map(removePunctuation) %>%
    tm_map(removeNumbers) %>%
    tm_map(stripWhitespace) %>%
    tm_map(removeWords, stopwords("en")) %>%
    tm_map(removeWords, stopwords("SMART")) %>%
    tm_map(removeWords, words)
  return(corpus)
}


# this function is written following the tutorial: https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling and course tutorials

top_terms_by_topic_LDA = function(text, plot = T, number_of_topics = 4) {    
  corpus = getCorpus(text)
  DTM = DocumentTermMatrix(corpus) 
  
  unique_indexes = unique(DTM$i) 
  DTM = DTM[unique_indexes,] 
  
  lda = LDA(DTM, k = number_of_topics, method = "Gibbs", control = list(iter = 2000,  thin = 400, nstart = 5, best = T, seed = list(1234, 423, 2211, 1122, 3345)))
  topics = tidy(lda, matrix = "beta")
  
  top_terms = topics %>% 
    group_by(topic) %>% 
    top_n(10, beta) %>% 
    ungroup() %>% 
    arrange(topic, -beta)
  
  if(plot == T){
    top_terms %>% 
      mutate(term = reorder(term, beta)) %>% 
      ggplot(aes(term, beta, fill = factor(topic))) + 
      geom_col(show.legend = FALSE) + 
      facet_wrap(~ topic, scales = "free") + 
      labs(x = NULL, y = "Topic concentration (beta)") + 
      coord_flip()
  }else{ 
    return(top_terms)
  }
}

# this function is written following the tutorial: https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling 

top_terms_by_topic_tfidf = function(text_df, text_column, group_column, plot = T) {
  group_column = enquo(group_column)
  text_column = enquo(text_column)
  
  words = text_df %>%
    unnest_tokens(word, !!text_column) %>%
    count(!!group_column, word) %>% 
    ungroup()
  
  total_words = words %>% 
    group_by(!!group_column) %>% 
    summarize(total = sum(n))
  
  words = left_join(words, total_words)
  
  tf_idf = words %>%
    bind_tf_idf(word, !!group_column, n) %>%
    select(-total) %>%
    arrange(desc(tf_idf)) %>%
    mutate(word = factor(word, levels = rev(unique(word))))
  
  if(plot == T){
    group_name <- quo_name(group_column)
    
    tf_idf %>% 
      group_by(!!group_column) %>% 
      top_n(10) %>% 
      ungroup %>%
      ggplot(aes(word, tf_idf, fill = as.factor(group_name))) +
      geom_col(show.legend = F) +
      labs(x = NULL, y = "tf-idf") +
      facet_wrap(reformulate(group_name), scales = "free") +
      coord_flip()
  }else{
    return(tf_idf)
  }
}

```

Reading data from on-line location.

```{r data, warning=FALSE, message=FALSE}

text = read_csv('https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv')
demog = read_csv('https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv')

```

Getting a look of data.

```{r}

summary(text)
```

As we see from the data above, the distribution of number of sentences is very skewed, and this is the long tail I'll be interested in.   

```{r}
ggplot(text, aes(x = num_sentence)) + geom_histogram(bins = 30)
```

Cleaning demographic dataframe.

```{r}

summary(demog)

```

As demographic data is essential for first part of the analysis, it needs some cleaning. I change "age" to numeric variable, as I bin by age in the analysis. I substitute erroneous inputs using "educated guess" logic and averaging - whatever is more appropriate in every case. I also convert "character" type to "factor" to see the skewness of the distribution. 

```{r warning=FALSE, message=FALSE}

demog$age = as.numeric(demog$age) 
demog = na.omit(demog)

demog %>%
  filter(age > 100 | age < 10) 

demog$age[demog$wid == "532"] = 23
demog$age[demog$wid == "1986"] = 27
demog$age[demog$wid %in% c("62", "2369", "2901", "3554", "12446")] = round(mean(demog$age), 0)

cols = c("country", "gender", "marital", "parenthood")
demog[cols] = lapply(demog[cols], factor)

summary(demog)

```

While "age", "country", "gender", "marital" and, "parenthood" look pretty balanced in the dataset, "country" is seriously biased in favor of USA.

### Part I. Exploratory Data Analysis on Demographic Data.  

As a person is a unit of the analysis, it makes sense to average lengths of all "happy moments" by person. This statistic - mean number of sentences per person - is the variable in question. Binning is performed for age for clarity of visualizations.

```{r}

text = data.table(text)
avg_len = text[, .(avg_sent_length = round(mean(num_sentence), 0)), by = wid]
demog = left_join(demog, avg_len, by = 'wid')
demog = na.omit(demog)
demog = mutate(demog, age_bin = cut(age, breaks = c(0,20,30,40,50,Inf), labels = c(">20", "21-30", "31-40", "41-50", "50>")))
summary(demog)

```

Let's consider intrinsic qualities as possible triggers for garrulous writing behavior, when it comes to happiness. 

```{r}
demog %>%
  filter(avg_sent_length >5) %>%
  ggplot(aes(x = age_bin, y = avg_sent_length, col = gender)) + geom_boxplot() + scale_y_log10() + xlab("Age") + ylab("# Sentences") + ggtitle("Garrelous people: Intrinsic Qualities")

```

Some interesting patterns are revealed. People are eager to tell more in their 20s and 40s, but they are less talkative about their happiness in between (I would not use "concise" as there all of them produced over 5 sentences in average). This is true for males and females, however, change for women seem more extreme.  I'm a little cautious to make conclusions about people below 20 and above 50 years old as sample is not well balanced, but general "ups and downs" pattern is clearly seen. If I'm looking for people motivated to write a long essay about their happy moments, my best candidate will likely be a female in her 40s or I can have some luck with people on their 20s.   

Now it is worthwhile to check extrinsic predictors. Due to insufficient representation of all marital status categories, the boxplot below is limited to major categories: "married" and "single". 

```{r}
demog %>%
  filter(avg_sent_length >5 & marital %in% c("married","single")) %>%
  ggplot(aes(x = marital, y = avg_sent_length, col = parenthood)) + geom_boxplot() + scale_y_log10() + xlab("Marital Status") + ylab("# Sentences") + ggtitle("Garrelous people: Extrinsic Qualities")

```

When it comes to family, married people with no kids can write an essay about their happiness. As kids join the family, chances are parents will cut their "happy stories". Interestingly, median for married writers with kids and single writers with or without kids opting for over 5 sentences is the same.

Final extrinsic feature to explore is "country". Even though USA is by far most frequent choice, and the representation is uneven, map view might be unformative as I'm interested in extremes. 

```{r map data, , warning=FALSE, message=FALSE}

demog_ISO3 = demog %>% 
  select(country, avg_sent_length) %>%
  group_by(country) %>%
  summarize(mean_len = mean(avg_sent_length), na.rm = T)

colourPalette = brewer.pal(8,'Oranges')

sPDF = joinCountryData2Map(demog_ISO3, joinCode = "ISO3", nameJoinColumn = "country", mapResolution = "coarse")
classInt = classIntervals( sPDF[["mean_len"]] ,n=8, style = "jenks")
catMethod = classInt[["brks"]]

mapCountryData(sPDF,
               nameColumnToPlot='mean_len', 
               mapTitle = "Number of Happy Sentances (average)", 
               numCats = 8, 
               catMethod = catMethod,
               colourPalette = colourPalette)

```

Verbose people live in Ukraine, Pakistan. There are a couple of bright spots to notice: Bangladesh, Nepal and Costa Rica. 

Before diving into text mining, I'd like to make use of category predictions present in the "text" dataset with regards to sentence number. In this analysis I continue looking at extreme  cases, i.e. those who writes a lot (6 sentences and more).

```{r}

garrulous = text %>%
  filter(num_sentence >5)

plot1 = beeswarm(num_sentence ~ predicted_category, data = garrulous, log=T, vertical = T, pch = 16, col = rainbow(8), method = 'hex', corral = "gutter", xlab = "Predicted Topic", ylab = "Number of Sentences", labels = c("Achieve", "Affect", "Bond", "Moment", "Exercise", "Leisure", "Nature"), main = "Garrelous people are happy about:")

```

Most popular topic for long essays has something to do with affection in the first place. Then come describing a moment, achievement or bonding. There are no particularly verbose descriptions on exercise-related "happy moments"; few are about leisure. It might be that describing more abstract categories, such as feelings requires more text than more concrete topics, such as "exercise". 

### Part II. Text mining.

##Unsupervised: LDA

Latent Dirichlet allocation helps to find a combination of words that is associated with a given number of topics. It is unsupervised method based on number of topics chosen by the requestor. In this analysis I treat a collection of "happy_moments" by verbosity as a document that contains a number of topics in order to see how the different topics are. 

```{r}

garrulous = text %>%
  filter(num_sentence >5) %>%
  select(cleaned_hm)  

top_terms_by_topic_LDA(garrulous, plot = T, number_of_topics = 2)

```

It is pretty hard to compose a story from the output as "topics" are filtered by the only factor - verbosity; a lot of separate inputs are fed into the model, so those representations are collective images. 

It is noticable that a generic word "life" has the highest scores. According to the output, people write long verbatimes about spending time with their family and friends as they come back home and about starting new things and socializing during the day.  

```{r}

taciturn = text %>%
  filter(num_sentence <3) %>%
  select(cleaned_hm)  

top_terms_by_topic_LDA(taciturn, plot = T, number_of_topics = 2)

```

People, who wrote concise descriptions of their "happy moments" seem to be more to the point: highest scores are awarded to more informative notions, such as "morning", "work", "girlfriend" and "friend". First collective image of happiness describes more concrete activities, such as "ate", "won", "met" and well-defined notions, such as "food" and "video". Second one is a composition of both family- and work-related socialization experiences.

It can be concluded that it doesn't matter how long an essay is - overlap in topics is pretty significant. Both subsets contain indication of social roles, times of the day and food-related experiences. The only difference is taciturn people operate more utilitarian vocabulary. Moreover, "Work" and "Job" is present only in concise statements. 

It is also interesting to compare distribution of beta-values common words in two categories in question.

```{r}

garrulous_LDA = top_terms_by_topic_LDA(garrulous, plot = F, number_of_topics = 2)
taciturn_LDA = top_terms_by_topic_LDA(taciturn, plot = F, number_of_topics = 2)

garrulous_LDA = garrulous_LDA %>%
  mutate(verbosity = "garrulous")
taciturn_LDA = taciturn_LDA %>%
  mutate(verbosity = "taciturn")

LDA_merged = rbind(garrulous_LDA, taciturn_LDA)

LDA_merged %>%
  filter(term %in% c("friends", "family", "home")) %>%
  select(term, beta, verbosity) %>%
  group_by(term) %>%
  arrange(-beta) %>%
  ggplot(aes(x = term, y = beta, fill = verbosity)) + geom_col(position ="dodge") + theme(axis.text=element_text(size=10))

```

According to the model, "family" and "home" gain bigger weight in terms of toic concentration among verbose people; and "friends" are scored higher among those, who describe their "happy moments" in fewer words. This is not an extreme dichotomy, by implies that outgoing people tend to write shorter essays than people focused on family issues. 

## Supervised: topic modeling with TF-IDF

Term Frequency - Inverse Document Frequency method allows to analyze a text in a smart way, as it decreases the weight for commonly used words and increases the weight for words that are not used very much across observations (Silge, 2018). Advantage of looking into the data with this technique is ability to use labels. As cultural differences might be very distinguisable in the essays, we can furhter look into "by country" splits for garrulous and taciturn people. 

This part of the analysis focus on four countries: 
- USA and India (as they are richly respresented in the data)
- Ukraine and Pakistan (as they represent most garrulous samples of essays)

```{r}

text_demog = left_join(text, demog, by = 'wid')
verbat = text_demog %>%
  select(cleaned_hm) 
corpus = getCorpus(verbat)
verbat_processed = data.frame(text=sapply(corpus, `[[`, "content"), stringsAsFactors=FALSE)
text_demog = cbind(text_demog, verbat_processed)

garrulous = text_demog %>%
  filter(country %in% c("USA", "IND", "PAK", "UKR")) %>%
  filter(num_sentence >5) %>%
  select(c(X1, country))

taciturn = text_demog %>%
  filter(country %in% c("USA", "IND", "PAK", "UKR")) %>%
  filter(num_sentence <3) %>%
  select(c(X1, country))

```

Using pre-defined function, complosing two dataframes with tf-idf scores.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
garrulous_tfidf = top_terms_by_topic_tfidf(text_df = garrulous, 
                         text_column = X1,
                         group_column = country, 
                         plot = F) 

taciturn_tfidf = top_terms_by_topic_tfidf(text_df = taciturn, 
                         text_column = X1,
                         group_column = country, 
                         plot = F) 
```

Plotting the results.

```{r}

garrulous_tfidf  %>% 
  group_by(country) %>% 
  top_n(5) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = country)) +
  geom_col(show.legend = F) +
  labs(x = NULL, y = "TF-IDF") +
  facet_wrap(~country, ncol = 4, scales = "free") +
  coord_flip() + 
  theme(axis.text=element_text(size=8)) +
  scale_fill_manual(values=c("#CC6666", "#9999CC", "#66CC99", "#CCCC33"))

```
India and USA are very similar as people write long texts about friends and family, life in general. Interestingly, "roti" is among most descriptive words in the USA group. This word might carry a meaning of a round flatbread in Indian culture, so it can be assumed that a fair portion of people identifying their country as "USA" might be Indian by origin. 

In case of Pakistan words in the output refer to very specific spiritual notions, such as "abhai" (can mean "no fear" or be the name of spiritual organization), "hari" (name of a sacred text). 

Ukrainian authors of verbose essays are very specific in describing their happy moment. Top words hint towards political topic (situation between Russia and Ukraine), social networking ("vkontakte" is a large social network in Russian-speaking community), work-related happy moments. Language is involved and specific.

```{r}

taciturn_tfidf  %>% 
  group_by(country) %>% 
  top_n(5) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = country)) +
  geom_col(show.legend = F) +
  labs(x = NULL, y = "TF-IDF") +
  facet_wrap(~country, ncol = 4, scales = "free") +
  coord_flip() + 
  theme(axis.text=element_text(size=8)) +
  scale_fill_manual(values=c("#CC6666", "#9999CC", "#66CC99"))

```

Looking at the subset of short descriptions of happy moments, first thing to notice is that people in Ukraine do not write short descriptions, all their stories are over 2 sentences. American and Indian essays again show a fair share of similarity. Those are pretty general words, some of them are common ("son"). Some specific notions appear in contrast to verbose descriptions ("movie", "game", "temple").

Interesting insights about short descriptions come from Pakistan. Some of them are actually misspellings. For example, "labtop" is a common misspelling for "laptop". Another thing, spirituality goes away; concrete and practical notions prevail.   

### Summary.

The goal of this analysis was to shed light on what makes people be verbose on describing their happy moments, how they are different form the rest of the crowd and whether the topics they address are different. 

The collective image of a garrulous person as it can be implied from the analysis in the following. It is an individual in his/her 20s and 40s (no dramatic difference in gender), married, yet, with no kids. Best countries to look for verbose writers are Ukraine and Pakistan. Happy Ukrainians are politically engaged, social and concentrated on their work, Pakistanis people, on the other hand, can produce long essays with spiritual flavor. "Home" and "family" may be a better indication of verbosity, concise statements tend to be a feature of outgoing people. 

There is some indication of verbose writers to using more abstract notions versus more actual notions based on LDA analysis. This claim may be also supported by distribution of long verbatims by "predicted category" that was included in the initial dataset: few people write long contributions about such concrete category as "exercise"; "affection" is the most popular category among longest descriptions.

###Reference

1. Julia Silge, David Robinson. Text Mining with R. 2018. https://www.tidytextmining.com 
2. Text Mining Tutorial. https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling 
3. Andy South. rworldmap: A New R package for Mapping Global Data. https://journal.r-project.org/archive/2011-1/RJournal_2011-1_South.pdf 
4. Wikipedia, http://www.abhai.org.in, https://www.urbandictionary.com/define.php?term=labtop for words interpretation.  

